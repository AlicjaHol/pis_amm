---
title: "Raport"
author: "Alicja Hołowiecka, Matylda Jankowska, Marcin Dziadosz"
date: "23 12 2019"
output:

  html_document: 
    toc: true
    toc_depth: 4
    theme: united
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

## Wstęp

W tym raporcie przeanalizujemy dwa szeregi czasowe: notowania firm McDonald's oraz Starbucks z okresu dwóch lat (od początku 2018 do końca 2019). Na potrzeby oceny w raporcie pojawia się nie tylko sama analiza, ale też wszystkie polecenia w języku `R`, jakich używaliśmy w jej celu.

## McDonald's

### Opis firmy

McDonald's to największa na świecie sieć restauracji szybkiej obsługi. Obejmuje ona ponad 30 tys. restauracji, każdego dnia obsługujących ponad 46 mln osób w 119 krajach. Wartość marki McDonald's szacuje się na 24,7 mld dolarów.

### Wczytanie danych i rysunki

Na początek wczytujemy bibliotekę `tseries`, która będzie nam potrzebna do wykonania analizy szeregu czasowego.

```{r}
library(tseries)
```

Dane pobieramy z `yahoo finance` za pomocą funkcji `get.hist.quote` i zamieniamy na typ numeryczny.

```{r}
mcd<- get.hist.quote(instrument = "MCD", provider = "yahoo",
                          quote = "Close", start = "2018-01-01", end = "2019-12-31")
mcd <- as.numeric(mcd)
```

Wykonamy rysunek przedstawiający notowania firmy McDonald's od 01-01-2018 do 31-12-2019

```{r}
plot(mcd, type = "l", xlab = "czas", ylab = "USD", main = "Notowania McDonald's")
```
 
Na rysunku w ciągu tych dwóch lat wyraźnie widać trend rosnący.

### Dopasowanie wielomianu

Spróbujemy do danych dopasować wielomian stopnia 1, 2 i 3.


```{r}
t <- 1:length(mcd)
```

#### Model liniowy

```{r}

mod1 <- lm(mcd~t)
summary(mod1)
```

Zarówno wyraz wolny, jak i współczynnik kierunkowy są istotne statystycznie. $R^2$ wynosi około 74%.

```{r}
par(mfrow = c(1, 2))
plot(mcd, type = "l", main = "Model liniowy", xlab = "czas", ylab = "USD")
abline(mod1, col = "red")
plot(mod1$residuals, type = "l", main = "Reszty modelu liniowego", xlab = "czas", ylab = "reszty")
abline(h=0)
par(mfrow = c(1, 1))
```

#### Model kwadratowy

Teraz stworzymy model kwadratowy.

```{r}
mod2 <- lm(mcd~t+I(t^2))
summary(mod2)
```

Wszystkie współczynniki są istotne statystycznie. $R^2$ wynosi około 75%, a więc zmieniło się bardzo nieznacznie.

```{r}
par(mfrow = c(1, 2))
plot(mcd, type = "l", main = "Model kwadratowy", xlab = "czas", ylab = "USD")
lines(t, mod2$fitted.values, col = "red", )
plot(mod2$residuals, type = "l", main = "Reszty modelu kwadratowego", xlab = "czas", ylab = "reszty")
abline(h = 0)
par(mfrow = c(1, 1))
```

Model kwadratowy zachowuje się bardzo podobnie jak model liniowy.

#### Model sześcienny



```{r}
mod3 <- lm(mcd~t+I(t^2)+I(t^3))
summary(mod3)
```

W modelu sześciennym wszystkie współczynniki są istotne statystycznie. $R^2$ wynosi 90%, a więc znacząco się poprawił w stosunku do poprzednich dwóch modeli.

```{r}
par(mfrow = c(1, 2))
plot(mcd, type = "l", main = "Model sześcienny", xlab = "czas", ylab = "USD")
lines(t, mod3$fitted.values, col = "red")
plot(mod3$residuals, type = "l", main = "Reszty modelu sześciennego", xlab = "czas", ylab = "reszty")
abline(h= 0)

```

Widać, że reszty modelu mają mniejszy rozrzut niż poprzednio - teraz mamy skalę od -15 do 15, a wcześniej było od -20 do 20.

#### Model z czwartą potęgą

```{r}
mod4 <- lm(mcd~t+I(t^2)+I(t^3)+I(t^4))
summary(mod4)
```

```{r}
par(mfrow = c(1, 2))
plot(mcd, type = "l", main = "Model z czwartą potęgą", xlab = "czas", ylab = "USD")
lines(t, mod4$fitted.values, col = "red")
plot(mod3$residuals, type = "l", main = "Reszty modelu z czwartą potęgą", xlab = "czas", ylab = "reszty")
abline(h= 0)
```

W modelu z czwartą potęgą współczynnik przy $t^3$ jest nieistotny statystycznie, ale nie możemy go usunąć, ponieważ efekt wyższego rzędu ($t^4$) jest istotny. $R^2$ wynosi około 91%, więc niewiele się różni od modelu sześciennego. Reszty także znajdują się w podobnym przedziale jak w poprzednim modelu. Model z $t^4$ niewiele się różni od tego z $t^3$, dlatego do dalszych badań wykorzystamy model sześcienny.

### Testy jednorodności wariancji reszt

Aby zbadać czy reszty w modelu są homoskedastyczne posłużymy się kilkoma popularnymi testami.

#### Test Breuscha-Pagana

$H_0:$ jednorodność wariancji reszt.
$H_1:$ wariancja reszt zależy od zmiennych objaśniających w modelu 

```{r}
library(lmtest)
pv1 <- bptest(mod3)$p.value
```

P-value wynosi `r sprintf("%.10f",pv1)`, należałoby zatem odrzucić hipotezę o jednorodności wariancji reszt.

#### Test Goldfelda-Quandta

Weryfikacja hipotezy polega na podziale danych na dwie grupy i sprawdzeniu, czy w obu wariancja ma taką samą wartość.

```{r}
pv2 <- gqtest(mod3, order.by = ~fitted(mod3))$p.value
```

P-value wynosi `r sprintf("%.3f", pv2)`, zatem nie ma podstaw do odrzucenia hipotezy o równości wariancji.

#### Test Harrisona-McCabe'a

Sprawdza hipotezę podobną do tej, którą weryfikuje test Goldfelda-Quandta; jednak w tym przypadku porównuje pierwszą połowę wartości do całości danych.

```{r}
hmctest(mod3, order.by = ~fitted(mod3))
pv3 <- hmctest(mod3, order.by = ~fitted(mod3))$p.value
```
P-value wynosi jest praktycznie równe 0, należy przyjąć hipotezę alternatywną, czyli wariancja reszt modelu ulega zmianie.

Biorąc pod uwagę uzyskane wyniki, należy przyjąć, że reszty z modelu trzeciego stopnia są heteroskedastyczne.

### Ruchoma średnia

Wykorzystamy metody ruchomych średnich, aby wygładzić szereg i zaobserwować ogólne trendy. Metoda średniej ruchomej ma na celu zmniejszenie rozrzutu razy $m+1$. 

W metodzie średniej ruchomej estymator części deterministycznej ma postać

$$\hat{f}(t) = \frac{1}{m+1}\sum_{k = 0}^{m}x_{t-k}$$

Do wykonania wygładzonych wykresów napisaliśmy funkcję `ruchoma`, której argumentami są `x` - szereg czasowy, `m` - paramter metody średniej ruchomej, `kolor` - kolor, na jaki dorysujemy wygładzoną linię na wykresie.

```{r}
ruchoma <- function(x, m, kolor){
  t <- length(x)
  f <- NULL
  for(i in (m+1):t){
    f[i] <- mean(x[(i-m):i])
  }
  plot(x, type = "l")
  lines((m+1):t, f[(m+1):t], lwd = 2, col = kolor)
}
```

Narysujemy wykresy dla kilku parametrów `m`.

Dla `m` = 3:

```{r}
ruchoma(mcd, 3, "red")
```

Dla `m` = 10:

```{r}
ruchoma(mcd, 10, "green")
```

Dla `m` = 30:

```{r}
ruchoma(mcd, 30, "blue")
```

Jak widać, im większy parametr `m` przyjmiemy, tym bardziej wygładzony wykres uzyskujemy, ale też mniej dokładny.


### Metoda wykładniczych wag ruchomej średniej

W metodzie ruchomej średniej obserwacje starsze i nowsze mają taką samą wagę, dlatego ta metoda jest mało dokładna. Skorzystamy teraz z dokładniejszej metody wykładniczych wag ruchomej średniej.

W tej metodzie estymator części deterministycznej ma postać:

$$\hat{f}(t) = \frac{1 - \eta}{1-\eta^t}\sum_{k = 0}^{t-1}\eta^kx_{t-k}$$

gdzie $\eta \in (0, 1)$

Skorzystamy z postaci rekurencyjnej:

$$\hat{f}(t)=\frac{1- \eta}{1 - \eta^t}\left[x_t+ \eta \frac{1 - \eta^{t-1}}{1 - \eta} \hat{f}(t-1) \right]$$

```{r}
wykladnicza <- function(x, mi, kolor){
  f <- NULL
  f[1] <- x[1]
  
  for (i in 2:length(x)){
    f[i] <- (1-mi)/(1-mi^i)*(x[i]+mi*(1-mi^(i-1))/(1-mi)*f[i-1])
  }
  plot(x, type = "l")
  lines(1:length(x), f, lwd = 2, col = kolor)
}
```

Dla $\eta = 0.2$


```{r}
wykladnicza(mcd, 0.2, "red")
```

Dla $\eta = 0.5$

```{r}
wykladnicza(mcd, 0.5, "green")

```

Dla $\eta = 0.7$

```{r}
wykladnicza(mcd, 0.7, "blue")
```

Dla $\eta = 0.9$

```{r}
wykladnicza(mcd, 0.9, "yellow")
```

Podobnie jak w przypadku prostej metody średniej ruchomej - im większy parametr $\eta$, tym bardziej wygładzony wykres, ale i mniejsza dokładność.

### Testy na resztach modelu

Do danych dobraliśmy wcześniej model wielomianowy trzeciego stopnia. Teraz sprawdzimy, czy reszty tego modelu spełniają założenia:

- losowość

- jednorodność wariancji

-normalność

- brak autokorelacji

#### Losowość

```{r}
library(randtests)
runs.test(mod3$residuals, threshold = 0, plot = T)
```
P-value bliskie zero, odrzucamy hipotezę o losowości reszt.

#### Normalność

```{r}
plot(density(mod3$residuals), main = "Wykres gęstości rozkładu reszt w porównaniu z rozkładem normalnym")
curve(dnorm(x, 0, sd(mod3$residuals)), add = T, col = 2, lwd = 2)
```

```{r}
qqnorm(mod3$residuals, main = "Wykres z linią kwantylową")
qqline(mod3$residuals, col=2, lwd = 3)
```

```{r}
plot(ecdf(mod3$residuals), main = "Dystrybuanta empiryczna w porównaniu z rozkładem normalnym")
curve(pnorm(x, 0, sd(mod3$residuals)), add = T, col = 2, lwd =2)
```

```{r}
library(nortest)
ks.test(x = mod3$residuals, y = "pnorm", mean = 0, sd = sd(mod3$residuals))
lillie.test(mod3$residuals)
shapiro.test(mod3$residuals)
ad.test(mod3$residuals)

```

Z testów Kołmogorowa-Lillieforsa, Shapiro-Wilka oraz Andersona-Darlinga wynika, że musimy odrzucić hipotezę o normalności rozkładu reszt (dla testu Kołmogorowa-Smirnova nie było podtsaw do odrzucenia, p-value około 0.14). Jeżeli chodzi o wykresy, to brak normalności najbardziej widać na wykresie gęstości. Na drugim wykresie (z linią kwantylową) reszty najbardziej odstają od rozkładu normalnego na początku i na końcu. Dystrybuanta empiryczna jest zbliżona do dystrybuanty rozkładu normalnego.

#### Autokorelacja

```{r}
acf(mod3$residuals)
```

Dla opóźnień do rzędu 25 obserwacje nie mieszczą się w niebieskich przerywanych liniach - wnioskujemy, że pojawia się autokorelacja.




### Metoda różnicowa

```{r}
par(mfrow = c(2, 3))
for(i in 1:6){
plot(diff(mcd, differences = i), type = "l")
abline(h = 0)}
par(mfrow=c(1,1))
```

### Stacjonarność

```{r}
adf.test(mcd) #niest
kpss.test(mcd) #niest
kpss.test(mcd, null = "Trend") #niest
adf.test(diff(mcd, differences = 1)) #stacj
kpss.test(diff(mcd, differences = 1)) #stacj
kpss.test(diff(mcd, differences = 1), null = "Trend") #stacj
```

Szereg jest niestacjonarny, i niestacjonarny względem trendu. Po zróżnicowaniu 1 raz, jest zarówno stacjonarny, jak i TS (Trend Stationary).

```{r}
library(forecast)
auto.arima(mcd)
```


### Inne rzeczy

Trend

W środowisku R dostępne są także funkcje dotyczące filtrowania szeregów czasowych. Jest to takie przekształcenie danych które doprowadza do oczyszczenia szeregu czasowego z wahań periodycznych. W środowisku R dostępnych jest kilka takich filtrów. Jeden z bardzie popularnych to filtr Hodrick-Prescotta zaimplementowany w pakiecie FRAPO::trdhp. Stosując filtr HP należy pamiętać o odpowiednim doborze parametru  
λ
 . Hodrick oraz Prescott zalecają, aby wartość współczynnika  
λ
  była równa 400, 1600 i 14400 odpowiednio dla danych rocznych, kwartalnych i miesięcznych.
  
  (P. Biecek Na przełaj przez Data Mining)
  
```{r}
library(FRAPO)
f <- FRAPO::trdhp(mcd, 14400)
plot(mcd, type = "l")
lines(f, col = 2)
plot(mcd - f, type = "l")


```



### Sezonowość

```{r}
mcd_sez <- ts(mcd, frequency = 12)
mcd_dek <- decompose(mcd_sez)
forecast:: ggseasonplot(mcd_sez)
forecast::ggseasonplot(mcd_sez, polar = T)
trend <- mcd_dek$trend
sezon <- mcd_dek$seasonal
reszty <- mcd_dek$random
plot(cbind(trend, sezon, reszty))
plot(stl(mcd_sez, "periodic"))
```

Chyba McDonalds jest jakiś super sezonowy, bo wszystko idealnie wygląda na wykresach... ;p

To skoro ten model wygląda najlepiej, to chyba dla jego reszt trzeba by testować... ?

```{r}
plot(reszty)
abline(h=0)
```


```{r}
shapiro.test(reszty) #ohohoo malutkie p value
library(lmtest)
qqnorm(reszty)
qqline(reszty, col = 2)
#nie ma normalności!!
```

```{r}
acf(reszty, na.action = na.pass) #dla dalszych z grubsza się mieszczą w pasku
```

### Arima

Dobra, ogólnie tutaj nie wiem jak to zrobić, czy nasz w końcu ma tę sezonowość? Bo jeśli ma, to SARIMA podobno, a jeśli nie to ARIMA. Plus, ten szereg chyba jest niestacjonarny, nie? To też przecież musi być stacjonarny i ja nie wiem już nic w końcu :((()))

```{r}
Arima(reszty, order = c(12,0,2)) #tu AIC jakoś 1842, jak sie leci z p do gory, to coraz lepiej, ale tez nie wiem co tu robic w ogóle
```

### Holt - Winters

```{r}
mcd_hw <- HoltWinters(mcd_sez, seasonal = "additive")
pred <- predict(mcd_hw, n.ahead = 4*12, prediction.interval = T, level = 0.9)
plot(mcd_hw, pred)
```

Predykcja za pomocą metody Holta-Wintersa. Testując dla różnej liczby okresów naprzód, widzimy, że przedział ufności drastycznie się rozszerza im większe `n.ahead`.
